{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f29bb35a",
   "metadata": {
    "id": "f29bb35a"
   },
   "source": [
    "# OpenAI OSS ADVANCED-fine-tuning by Trelis\n",
    "Advanced scripts available at [Trelis.com](https://Trelis.com/ADVANCED-fine-tuning)\n",
    "\n",
    "*Based on the [OpenAI cookbook notebook](https://cookbook.openai.com/articles/gpt-oss/fine-tune-transfomers).*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6d1290",
   "metadata": {
    "id": "3f6d1290"
   },
   "source": [
    "Large reasoning models like **OpenAI o3** generate a *chain‑of‑thought* to improve the accuracy and quality of their responses.  \n",
    "However, most of these models reason in English, even when a question is asked in another language.\n",
    "\n",
    "In this notebook, we show how the open‑weight reasoning model **`openai/gpt-oss-20b`** can be fine‑tuned to reason effectively in multiple languages.  \n",
    "We'll add a new **“reasoning language”** option to the model’s system prompt and apply supervised fine‑tuning with Hugging Face’s **TRL** library on a multilingual reasoning dataset.\n",
    "\n",
    "**Outline**\n",
    "\n",
    "1. **Setup** – install libraries  \n",
    "2. **Prepare the dataset** – download & format  \n",
    "3. **Prepare the model** – load, quantize & LoRA‑wrap  \n",
    "4. **Fine‑tuning** – train with multilingual reasoning data  \n",
    "5. **Inference** – generate reasoning responses in different languages  \n",
    "\n",
    "When we're done you’ll have a multilingual reasoning model that can:  \n",
    "\n",
    "* reason in **English, Spanish, French, Italian, or German**,  \n",
    "* even mix languages – e.g. ask in Spanish, reason in German, answer in Spanish.\n",
    "\n",
    "> **Example**\n",
    "\n",
    "```\n",
    "User:\n",
    "    ¿Cuál es el capital de Australia?\n",
    "Assistant reasoning:\n",
    "    Okay, der Benutzer fragt nach der Hauptstadt Australiens. [...]\n",
    "Assistant response:\n",
    "    La capital de Australia es **Canberra**. [...]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6c7caf",
   "metadata": {
    "id": "1d6c7caf"
   },
   "source": [
    "## 1&nbsp;&nbsp;Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e165759e",
   "metadata": {
    "id": "e165759e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (25.2)\n"
     ]
    }
   ],
   "source": [
    "# Install PyTorch (CUDA 12.8 build)\n",
    "!python -m pip install --upgrade pip\n",
    "!pip install uv -qU\n",
    "\n",
    "# !pip show torch\n",
    "\n",
    "!uv pip install torch --index-url https://download.pytorch.org/whl/cu128 --system -q\n",
    "\n",
    "# Install remaining dependencies\n",
    "!uv pip install tensorboard hf_transfer huggingface_hub \"trl>=0.20.0\" \"peft>=0.17.0\" \"transformers>=4.55.0\" trackio --system -q\n",
    "\n",
    "import os\n",
    "os.environ[\"HF_TRANSFER\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ba04c76-98c3-4746-8af6-b505a5c29108",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfFolder, login\n",
    "\n",
    "# Check if a token is already saved\n",
    "if HfFolder.get_token() is None:\n",
    "    login()  # Will prompt only if not logged in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7ddd78",
   "metadata": {
    "id": "7b7ddd78"
   },
   "source": [
    "## 2&nbsp;&nbsp;Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6383597",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b6383597",
    "outputId": "99315740-9e24-4098-ec59-82be4dccefea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 968\n",
      "Validation size: 32\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load full dataset\n",
    "dataset = load_dataset(\"HuggingFaceH4/Multilingual-Thinking\", split=\"train\")\n",
    "\n",
    "# --- Optional validation split ---\n",
    "do_val_split = True  # Set to False to skip splitting\n",
    "\n",
    "if do_val_split:\n",
    "    # Determine number of rows to reserve for validation\n",
    "    val_size = min(int(0.1 * len(dataset)), 32)\n",
    "    val_size = min(val_size, len(dataset))  # ensure we don't exceed dataset size\n",
    "\n",
    "    # Shuffle and split\n",
    "    dataset = dataset.shuffle(seed=42)\n",
    "    val_dataset = dataset.select(range(val_size))\n",
    "    train_dataset = dataset.select(range(val_size, len(dataset)))\n",
    "else:\n",
    "    train_dataset = dataset\n",
    "    val_dataset = None\n",
    "\n",
    "# --- Output ---\n",
    "print(f\"Train size: {len(train_dataset)}\")\n",
    "if val_dataset:\n",
    "    print(f\"Validation size: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94e52621",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "94e52621",
    "outputId": "d3202759-03a6-4d1e-fcca-d17d6e9fb1bd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['reasoning_language', 'developer', 'user', 'analysis', 'final', 'messages'],\n",
       "    num_rows: 968\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the first training example\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ccc494e-4ba0-4bd2-87a6-8ea99f9c3c23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['reasoning_language', 'developer', 'user', 'analysis', 'final', 'messages'],\n",
       "    num_rows: 32\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c13886e",
   "metadata": {
    "id": "6c13886e"
   },
   "source": [
    "The **gpt‑oss** models use the *Harmony* response format to structure conversations:\n",
    "\n",
    "| role       | purpose                                                         |\n",
    "|------------|-----------------------------------------------------------------|\n",
    "| developer  | custom system instructions                                      |\n",
    "| user       | user input                                                      |\n",
    "| assistant  | tool calls or responses                                         |\n",
    "| analysis   | chain‑of‑thought                                                |\n",
    "| final      | final answer for the end‑user                                   |\n",
    "\n",
    "We convert these messages with `tokenizer.apply_chat_template()` so the model understands them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d195ad13",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d195ad13",
    "outputId": "e6e8f42b-4906-4b40-8096-02405652ead0"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai/gpt-oss-20b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f294e9b3-1e1c-4057-a418-05ad3f6cacc7",
   "metadata": {},
   "source": [
    "More details on prompt formatting [here](https://cookbook.openai.com/articles/openai-harmony).\n",
    "\n",
    "**Purpose**\n",
    "\n",
    "*A Jinja2-based template to format chat messages for an LLM that supports:*\n",
    "- *Chain-of-thought reasoning*\n",
    "- *Tool usage (e.g., browser, Python)*\n",
    "- *Multilingual support*\n",
    "\n",
    "**Core Components**\n",
    "\n",
    "**1. System Message Generation (`build_system_message`)**\n",
    "\n",
    "*Adds model metadata:*\n",
    "- *`model_identity`*\n",
    "- *`Knowledge cutoff`*\n",
    "- *`Current date`*\n",
    "- *`Reasoning effort`*\n",
    "- *Renders tool sections if tools are available*\n",
    "\n",
    "**2. Tool Rendering Macros**\n",
    "\n",
    "- *`render_tool_namespace`: formats user-defined tools*\n",
    "- *`render_builtin_tools`: supports built-ins like `browser`, `python`*\n",
    "- *Uses TypeScript-style function signature rendering*\n",
    "\n",
    "**3. Message Rendering Logic**\n",
    "\n",
    "*Wraps each message with structured tags:*\n",
    "```text\n",
    "<|start|>role<|channel|>type<|message|>...<|end|>\n",
    "```\n",
    "*Channels include:*\n",
    "- *`analysis`*\n",
    "- *`commentary`*\n",
    "- *`final`*\n",
    "*Supports roles: `user`, `assistant`, `tool`, `developer`*\n",
    "\n",
    "**4. Tool Call State Tracking**\n",
    "\n",
    "*Tracks the most recent tool name via `last_tool_call`*\n",
    "*Links tool outputs to the correct assistant call*\n",
    "\n",
    "**5. Helper Macros**\n",
    "\n",
    "- *`render_typescript_type`: handles rendering of complex tool param types*\n",
    "\n",
    "**6. Validation Rules**\n",
    "\n",
    "*Throws exceptions if formatting rules are broken:*\n",
    "- *Mixing `<|channel|>` inside message content*\n",
    "- *Providing both `thinking` and `content` in assistant messages with tools*\n",
    "\n",
    "**7. Inference Prompt Ending**\n",
    "\n",
    "*Optionally appends:*\n",
    "```text\n",
    "<|start|>assistant\n",
    "```\n",
    "*to signal the model to g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c655709-6d9b-4297-89ac-e68bf69acdda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```jinja2\n",
       "{#-\n",
       "  In addition to the normal inputs of `messages` and `tools`, this template also accepts the\n",
       "  following kwargs:\n",
       "  - \"builtin_tools\": A list, can contain \"browser\" and/or \"python\".\n",
       "  - \"model_identity\": A string that optionally describes the model identity.\n",
       "  - \"reasoning_effort\": A string that describes the reasoning effort, defaults to \"medium\".\n",
       " #}\n",
       "\n",
       "{#- Tool Definition Rendering ============================================== #}\n",
       "{%- macro render_typescript_type(param_spec, required_params, is_nullable=false) -%}\n",
       "    {%- if param_spec.type == \"array\" -%}\n",
       "        {%- if param_spec['items'] -%}\n",
       "            {%- if param_spec['items']['type'] == \"string\" -%}\n",
       "                {{- \"string[]\" }}\n",
       "            {%- elif param_spec['items']['type'] == \"number\" -%}\n",
       "                {{- \"number[]\" }}\n",
       "            {%- elif param_spec['items']['type'] == \"integer\" -%}\n",
       "                {{- \"number[]\" }}\n",
       "            {%- elif param_spec['items']['type'] == \"boolean\" -%}\n",
       "                {{- \"boolean[]\" }}\n",
       "            {%- else -%}\n",
       "                {%- set inner_type = render_typescript_type(param_spec['items'], required_params) -%}\n",
       "                {%- if inner_type == \"object | object\" or inner_type|length > 50 -%}\n",
       "                    {{- \"any[]\" }}\n",
       "                {%- else -%}\n",
       "                    {{- inner_type + \"[]\" }}\n",
       "                {%- endif -%}\n",
       "            {%- endif -%}\n",
       "            {%- if param_spec.nullable -%}\n",
       "                {{- \" | null\" }}\n",
       "            {%- endif -%}\n",
       "        {%- else -%}\n",
       "            {{- \"any[]\" }}\n",
       "            {%- if param_spec.nullable -%}\n",
       "                {{- \" | null\" }}\n",
       "            {%- endif -%}\n",
       "        {%- endif -%}\n",
       "    {%- elif param_spec.type is defined and param_spec.type is iterable and param_spec.type is not string and param_spec.type is not mapping and param_spec.type[0] is defined -%}\n",
       "        {#- Handle array of types like [\"object\", \"object\"] from Union[dict, list] #}\n",
       "        {%- if param_spec.type | length > 1 -%}\n",
       "            {{- param_spec.type | join(\" | \") }}\n",
       "        {%- else -%}\n",
       "            {{- param_spec.type[0] }}\n",
       "        {%- endif -%}\n",
       "    {%- elif param_spec.oneOf -%}\n",
       "        {#- Handle oneOf schemas - check for complex unions and fallback to any #}\n",
       "        {%- set has_object_variants = false -%}\n",
       "        {%- for variant in param_spec.oneOf -%}\n",
       "            {%- if variant.type == \"object\" -%}\n",
       "                {%- set has_object_variants = true -%}\n",
       "            {%- endif -%}\n",
       "        {%- endfor -%}\n",
       "        {%- if has_object_variants and param_spec.oneOf|length > 1 -%}\n",
       "            {{- \"any\" }}\n",
       "        {%- else -%}\n",
       "            {%- for variant in param_spec.oneOf -%}\n",
       "                {{- render_typescript_type(variant, required_params) -}}\n",
       "                {%- if variant.description %}\n",
       "                    {{- \"// \" + variant.description }}\n",
       "                {%- endif -%}\n",
       "                {%- if variant.default is defined %}\n",
       "                    {{ \"// default: \" + variant.default|tojson }}\n",
       "                {%- endif -%}\n",
       "                {%- if not loop.last %}\n",
       "                    {{- \" | \" }}\n",
       "                {% endif -%}\n",
       "            {%- endfor -%}\n",
       "        {%- endif -%}\n",
       "    {%- elif param_spec.type == \"string\" -%}\n",
       "        {%- if param_spec.enum -%}\n",
       "            {{- '\"' + param_spec.enum|join('\" | \"') + '\"' -}}\n",
       "        {%- else -%}\n",
       "            {{- \"string\" }}\n",
       "            {%- if param_spec.nullable %}\n",
       "                {{- \" | null\" }}\n",
       "            {%- endif -%}\n",
       "        {%- endif -%}\n",
       "    {%- elif param_spec.type == \"number\" -%}\n",
       "        {{- \"number\" }}\n",
       "    {%- elif param_spec.type == \"integer\" -%}\n",
       "        {{- \"number\" }}\n",
       "    {%- elif param_spec.type == \"boolean\" -%}\n",
       "        {{- \"boolean\" }}\n",
       "\n",
       "    {%- elif param_spec.type == \"object\" -%}\n",
       "        {%- if param_spec.properties -%}\n",
       "            {{- \"{\n",
       "\" }}\n",
       "            {%- for prop_name, prop_spec in param_spec.properties.items() -%}\n",
       "                {{- prop_name -}}\n",
       "                {%- if prop_name not in (param_spec.required or []) -%}\n",
       "                    {{- \"?\" }}\n",
       "                {%- endif -%}\n",
       "                {{- \": \" }}\n",
       "                {{ render_typescript_type(prop_spec, param_spec.required or []) }}\n",
       "                {%- if not loop.last -%}\n",
       "                    {{-\", \" }}\n",
       "                {%- endif -%}\n",
       "            {%- endfor -%}\n",
       "            {{- \"}\" }}\n",
       "        {%- else -%}\n",
       "            {{- \"object\" }}\n",
       "        {%- endif -%}\n",
       "    {%- else -%}\n",
       "        {{- \"any\" }}\n",
       "    {%- endif -%}\n",
       "{%- endmacro -%}\n",
       "\n",
       "{%- macro render_tool_namespace(namespace_name, tools) -%}\n",
       "    {{- \"## \" + namespace_name + \"\n",
       "\n",
       "\" }}\n",
       "    {{- \"namespace \" + namespace_name + \" {\n",
       "\n",
       "\" }}\n",
       "    {%- for tool in tools %}\n",
       "        {%- set tool = tool.function %}\n",
       "        {{- \"// \" + tool.description + \"\n",
       "\" }}\n",
       "        {{- \"type \"+ tool.name + \" = \" }}\n",
       "        {%- if tool.parameters and tool.parameters.properties %}\n",
       "            {{- \"(_: {\n",
       "\" }}\n",
       "            {%- for param_name, param_spec in tool.parameters.properties.items() %}\n",
       "                {%- if param_spec.description %}\n",
       "                    {{- \"// \" + param_spec.description + \"\n",
       "\" }}\n",
       "                {%- endif %}\n",
       "                {{- param_name }}\n",
       "                {%- if param_name not in (tool.parameters.required or []) -%}\n",
       "                    {{- \"?\" }}\n",
       "                {%- endif -%}\n",
       "                {{- \": \" }}\n",
       "                {{- render_typescript_type(param_spec, tool.parameters.required or []) }}\n",
       "                {%- if param_spec.default is defined -%}\n",
       "                    {%- if param_spec.enum %}\n",
       "                        {{- \", // default: \" + param_spec.default }}\n",
       "                    {%- elif param_spec.oneOf %}\n",
       "                        {{- \"// default: \" + param_spec.default }}\n",
       "                    {%- else %}\n",
       "                        {{- \", // default: \" + param_spec.default|tojson }}\n",
       "                    {%- endif -%}\n",
       "                {%- endif -%}\n",
       "                {%- if not loop.last %}\n",
       "                    {{- \",\n",
       "\" }}\n",
       "                {%- else %}\n",
       "                    {{- \"\n",
       "\" }}\n",
       "                {%- endif -%}\n",
       "            {%- endfor %}\n",
       "            {{- \"}) => any;\n",
       "\n",
       "\" }}\n",
       "        {%- else -%}\n",
       "            {{- \"() => any;\n",
       "\n",
       "\" }}\n",
       "        {%- endif -%}\n",
       "    {%- endfor %}\n",
       "    {{- \"} // namespace \" + namespace_name }}\n",
       "{%- endmacro -%}\n",
       "\n",
       "{%- macro render_builtin_tools(browser_tool, python_tool) -%}\n",
       "    {%- if browser_tool %}\n",
       "        {{- \"## browser\n",
       "\n",
       "\" }}\n",
       "        {{- \"// Tool for browsing.\n",
       "\" }}\n",
       "        {{- \"// The `cursor` appears in brackets before each browsing display: `[{cursor}]`.\n",
       "\" }}\n",
       "        {{- \"// Cite information from the tool using the following format:\n",
       "\" }}\n",
       "        {{- \"// `【{cursor}†L{line_start}(-L{line_end})?】`, for example: `【6†L9-L11】` or `【8†L3】`.\n",
       "\" }}\n",
       "        {{- \"// Do not quote more than 10 words directly from the tool output.\n",
       "\" }}\n",
       "        {{- \"// sources=web (default: web)\n",
       "\" }}\n",
       "        {{- \"namespace browser {\n",
       "\n",
       "\" }}\n",
       "        {{- \"// Searches for information related to `query` and displays `topn` results.\n",
       "\" }}\n",
       "        {{- \"type search = (_: {\n",
       "\" }}\n",
       "        {{- \"query: string,\n",
       "\" }}\n",
       "        {{- \"topn?: number, // default: 10\n",
       "\" }}\n",
       "        {{- \"source?: string,\n",
       "\" }}\n",
       "        {{- \"}) => any;\n",
       "\n",
       "\" }}\n",
       "        {{- \"// Opens the link `id` from the page indicated by `cursor` starting at line number `loc`, showing `num_lines` lines.\n",
       "\" }}\n",
       "        {{- \"// Valid link ids are displayed with the formatting: `【{id}†.*】`.\n",
       "\" }}\n",
       "        {{- \"// If `cursor` is not provided, the most recent page is implied.\n",
       "\" }}\n",
       "        {{- \"// If `id` is a string, it is treated as a fully qualified URL associated with `source`.\n",
       "\" }}\n",
       "        {{- \"// If `loc` is not provided, the viewport will be positioned at the beginning of the document or centered on the most relevant passage, if available.\n",
       "\" }}\n",
       "        {{- \"// Use this function without `id` to scroll to a new location of an opened page.\n",
       "\" }}\n",
       "        {{- \"type open = (_: {\n",
       "\" }}\n",
       "        {{- \"id?: number | string, // default: -1\n",
       "\" }}\n",
       "        {{- \"cursor?: number, // default: -1\n",
       "\" }}\n",
       "        {{- \"loc?: number, // default: -1\n",
       "\" }}\n",
       "        {{- \"num_lines?: number, // default: -1\n",
       "\" }}\n",
       "        {{- \"view_source?: boolean, // default: false\n",
       "\" }}\n",
       "        {{- \"source?: string,\n",
       "\" }}\n",
       "        {{- \"}) => any;\n",
       "\n",
       "\" }}\n",
       "        {{- \"// Finds exact matches of `pattern` in the current page, or the page given by `cursor`.\n",
       "\" }}\n",
       "        {{- \"type find = (_: {\n",
       "\" }}\n",
       "        {{- \"pattern: string,\n",
       "\" }}\n",
       "        {{- \"cursor?: number, // default: -1\n",
       "\" }}\n",
       "        {{- \"}) => any;\n",
       "\n",
       "\" }}\n",
       "        {{- \"} // namespace browser\n",
       "\n",
       "\" }}\n",
       "    {%- endif -%}\n",
       "\n",
       "    {%- if python_tool %}\n",
       "        {{- \"## python\n",
       "\n",
       "\" }}\n",
       "        {{- \"Use this tool to execute Python code in your chain of thought. The code will not be shown to the user. This tool should be used for internal reasoning, but not for code that is intended to be visible to the user (e.g. when creating plots, tables, or files).\n",
       "\n",
       "\" }}\n",
       "        {{- \"When you send a message containing Python code to python, it will be executed in a stateful Jupyter notebook environment. python will respond with the output of the execution or time out after 120.0 seconds. The drive at '/mnt/data' can be used to save and persist user files. Internet access for this session is UNKNOWN. Depends on the cluster.\n",
       "\n",
       "\" }}\n",
       "    {%- endif -%}\n",
       "{%- endmacro -%}\n",
       "\n",
       "{#- System Message Construction ============================================ #}\n",
       "{%- macro build_system_message() -%}\n",
       "    {%- if model_identity is not defined %}\n",
       "        {%- set model_identity = \"You are ChatGPT, a large language model trained by OpenAI.\" %}\n",
       "    {%- endif %}\n",
       "    {{- model_identity + \"\n",
       "\" }}\n",
       "    {{- \"Knowledge cutoff: 2024-06\n",
       "\" }}\n",
       "    {{- \"Current date: \" + strftime_now(\"%Y-%m-%d\") + \"\n",
       "\n",
       "\" }}\n",
       "    {%- if reasoning_effort is not defined %}\n",
       "        {%- set reasoning_effort = \"medium\" %}\n",
       "    {%- endif %}\n",
       "    {{- \"Reasoning: \" + reasoning_effort + \"\n",
       "\n",
       "\" }}\n",
       "    {%- if builtin_tools %}\n",
       "        {{- \"# Tools\n",
       "\n",
       "\" }}\n",
       "        {%- set available_builtin_tools = namespace(browser=false, python=false) %}\n",
       "        {%- for tool in builtin_tools %}\n",
       "            {%- if tool == \"browser\" %}\n",
       "                {%- set available_builtin_tools.browser = true %}\n",
       "            {%- elif tool == \"python\" %}\n",
       "                {%- set available_builtin_tools.python = true %}\n",
       "            {%- endif %}\n",
       "        {%- endfor %}\n",
       "        {{- render_builtin_tools(available_builtin_tools.browser, available_builtin_tools.python) }}\n",
       "    {%- endif -%}\n",
       "    {{- \"# Valid channels: analysis, commentary, final. Channel must be included for every message.\" }}\n",
       "    {%- if tools -%}\n",
       "        {{- \"\n",
       "Calls to these tools must go to the commentary channel: 'functions'.\" }}\n",
       "    {%- endif -%}\n",
       "{%- endmacro -%}\n",
       "\n",
       "{#- Main Template Logic ================================================= #}\n",
       "{#- Set defaults #}\n",
       "\n",
       "{#- Render system message #}\n",
       "{{- \"<|start|>system<|message|>\" }}\n",
       "{{- build_system_message() }}\n",
       "{{- \"<|end|>\" }}\n",
       "\n",
       "{#- Extract developer message #}\n",
       "{%- if messages[0].role == \"developer\" or messages[0].role == \"system\" %}\n",
       "    {%- set developer_message = messages[0].content %}\n",
       "    {%- set loop_messages = messages[1:] %}\n",
       "{%- else %}\n",
       "    {%- set developer_message = \"\" %}\n",
       "    {%- set loop_messages = messages %}\n",
       "{%- endif %}\n",
       "\n",
       "{#- Render developer message #}\n",
       "{%- if developer_message or tools %}\n",
       "    {{- \"<|start|>developer<|message|>\" }}\n",
       "    {%- if developer_message %}\n",
       "        {{- \"# Instructions\n",
       "\n",
       "\" }}\n",
       "        {{- developer_message }}\n",
       "    {%- endif %}\n",
       "    {%- if tools -%}\n",
       "        {{- \"\n",
       "\n",
       "\" }}\n",
       "        {{- \"# Tools\n",
       "\n",
       "\" }}\n",
       "        {{- render_tool_namespace(\"functions\", tools) }}\n",
       "    {%- endif -%}\n",
       "    {{- \"<|end|>\" }}\n",
       "{%- endif %}\n",
       "\n",
       "{#- Render messages #}\n",
       "{%- set last_tool_call = namespace(name=none) %}\n",
       "{%- for message in loop_messages -%}\n",
       "    {#- At this point only assistant/user/tool messages should remain #}\n",
       "    {%- if message.role == 'assistant' -%}\n",
       "        {#- Checks to ensure the messages are being passed in the format we expect #}\n",
       "        {%- if \"content\" in message %}\n",
       "            {%- if \"<|channel|>analysis<|message|>\" in message.content or \"<|channel|>final<|message|>\" in message.content %}\n",
       "                {{- raise_exception(\"You have passed a message containing <|channel|> tags in the content field. Instead of doing this, you should pass analysis messages (the string between '<|message|>' and '<|end|>') in the 'thinking' field, and final messages (the string between '<|message|>' and '<|end|>') in the 'content' field.\") }}\n",
       "            {%- endif %}\n",
       "        {%- endif %}\n",
       "        {%- if \"thinking\" in message %}\n",
       "            {%- if \"<|channel|>analysis<|message|>\" in message.thinking or \"<|channel|>final<|message|>\" in message.thinking %}\n",
       "                {{- raise_exception(\"You have passed a message containing <|channel|> tags in the thinking field. Instead of doing this, you should pass analysis messages (the string between '<|message|>' and '<|end|>') in the 'thinking' field, and final messages (the string between '<|message|>' and '<|end|>') in the 'content' field.\") }}\n",
       "            {%- endif %}\n",
       "        {%- endif %}\n",
       "        {%- if \"tool_calls\" in message %}\n",
       "            {#- We assume max 1 tool call per message, and so we infer the tool call name #}\n",
       "            {#- in \"tool\" messages from the most recent assistant tool call name #}\n",
       "            {%- set tool_call = message.tool_calls[0] %}\n",
       "            {%- if tool_call.function %}\n",
       "                {%- set tool_call = tool_call.function %}\n",
       "            {%- endif %}\n",
       "            {%- if message.content and message.thinking %}\n",
       "                {{- raise_exception(\"Cannot pass both content and thinking in an assistant message with tool calls! Put the analysis message in one or the other, but not both.\") }}\n",
       "            {%- elif message.content %}\n",
       "                {{- \"<|start|>assistant<|channel|>analysis<|message|>\" + message.content + \"<|end|>\" }}\n",
       "            {%- elif message.thinking %}\n",
       "                {{- \"<|start|>assistant<|channel|>analysis<|message|>\" + message.thinking + \"<|end|>\" }}\n",
       "            {%- endif %}\n",
       "            {{- \"<|start|>assistant to=\" }}\n",
       "            {{- \"functions.\" + tool_call.name + \"<|channel|>commentary \" }}\n",
       "            {{- (tool_call.content_type if tool_call.content_type is defined else \"json\") + \"<|message|>\" }}\n",
       "            {{- tool_call.arguments|tojson }}\n",
       "            {{- \"<|call|>\" }}\n",
       "            {%- set last_tool_call.name = tool_call.name %}\n",
       "        {%- elif loop.last and not add_generation_prompt %}\n",
       "            {#- Only render the CoT if the final turn is an assistant turn and add_generation_prompt is false #}\n",
       "            {#- This is a situation that should only occur in training, never in inference. #}\n",
       "            {%- if \"thinking\" in message %}\n",
       "                {{- \"<|start|>assistant<|channel|>analysis<|message|>\" + message.thinking + \"<|end|>\" }}\n",
       "            {%- endif %}\n",
       "            {#- <|return|> indicates the end of generation, but <|end|> does not #}\n",
       "            {#- <|return|> should never be an input to the model, but we include it as the final token #}\n",
       "            {#- when training, so the model learns to emit it. #}\n",
       "            {{- \"<|start|>assistant<|channel|>final<|message|>\" + message.content + \"<|return|>\" }}\n",
       "        {%- else %}\n",
       "            {#- CoT is dropped during all previous turns, so we never render it for inference #}\n",
       "            {{- \"<|start|>assistant<|channel|>final<|message|>\" + message.content + \"<|end|>\" }}\n",
       "            {%- set last_tool_call.name = none %}\n",
       "        {%- endif %}\n",
       "    {%- elif message.role == 'tool' -%}\n",
       "        {%- if last_tool_call.name is none %}\n",
       "            {{- raise_exception(\"Message has tool role, but there was no previous assistant message with a tool call!\") }}\n",
       "        {%- endif %}\n",
       "        {{- \"<|start|>functions.\" + last_tool_call.name }}\n",
       "        {{- \" to=assistant<|channel|>commentary<|message|>\" + message.content|tojson + \"<|end|>\" }}\n",
       "    {%- elif message.role == 'user' -%}\n",
       "        {{- \"<|start|>user<|message|>\" + message.content + \"<|end|>\" }}\n",
       "    {%- endif -%}\n",
       "{%- endfor -%}\n",
       "\n",
       "{#- Generation prompt #}\n",
       "{%- if add_generation_prompt -%}\n",
       "<|start|>assistant\n",
       "{%- endif -%}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Wrap it in a code block for syntax highlighting\n",
    "display(Markdown(f\"```jinja2\\n{tokenizer.chat_template}\\n```\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16f7ea2b-d018-4048-afa3-071da5f9a820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\n",
      "Knowledge cutoff: 2024-06\n",
      "Current date: 2025-08-06\n",
      "\n",
      "Reasoning: medium\n",
      "\n",
      "# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>developer<|message|># Instructions\n",
      "\n",
      "reasoning language: English\n",
      "\n",
      "You are an AI chatbot that provides financial advice, but under no circumstances should you provide investment guarantees.<|end|><|start|>user<|message|>Can you suggest a healthy breakfast option that is less than 200 calories?<|end|><|start|>assistant<|channel|>analysis<|message|>\n",
      "Okay, the user is asking for a healthy breakfast under 200 calories. Let me start by thinking about common breakfast foods and their calorie counts.\n",
      "\n",
      "First, I need to consider the main components: proteins, carbs, and fats. Since the goal is low calories, I should focus on foods that are nutrient-dense but not high in calories.\n",
      "\n",
      "Maybe a breakfast with eggs? Scrambled eggs are good, but they can add up. Let me check the calories. Two large eggs are about 140 calories. If I add some veggies, that's good for fiber and vitamins without many calories.\n",
      "\n",
      "What about adding spinach and tomatoes? Those are low-calorie and add nutrients. Maybe a slice of whole-grain toast. Whole grain is better for sustained energy. A slice is around 70-80 calories. So 140 (eggs) + 80 (toast) = 220. That's over 200. Hmm, need to adjust.\n",
      "\n",
      "Wait, maybe use one egg instead of two? One egg is 70 calories. Then the toast is 80, so 150 total. Then add veggies. That's better. Also, maybe use a non-dairy milk like almond milk if adding any liquid. Or maybe skip the toast and use a lower-calorie base. \n",
      "\n",
      "Another option: Greek yogurt. Low-fat Greek yogurt is around 100 calories for a cup. Add some berries, which are low in calories and high in antioxidants. Maybe a sprinkle of nuts, but nuts are calorie-dense. Need to be careful. Maybe a few almonds or a teaspoon of chia seeds for fiber and omega-3s.\n",
      "\n",
      "Wait, let's calculate. 100 calories for yogurt, 50 for berries, 30 for chia seeds. That's 180. That works. Or maybe add a hard-boiled egg on top for extra protein. A hard-boiled egg is about 70 calories. Then total would be 100 + 70 + 50 = 220 again. So maybe skip the egg and stick with yogurt and berries.\n",
      "\n",
      "Alternatively, oatmeal. A half-cup of dry oats is about 150 calories. Cooked in water or milk. Add some fruit. Maybe a banana or apple. But that might push it over. Maybe half a banana is 50 calories. So total would be 150 + 50 = 200. That works. Or use a non-fat milk to cook the oats.\n",
      "\n",
      "Another option: smoothie. Use a banana, spinach, protein powder, and water. But protein powder can add calories. Maybe a low-calorie protein powder or just use the banana and spinach. Let me check. A banana is 100 calories, a cup of spinach is negligible, and maybe a splash of water. That's 100 calories. Then add some almond butter for healthy fats, but that's high in calories. Maybe skip it. So total around 100 calories, which is under. But maybe the user wants more protein.\n",
      "\n",
      "Wait, the user might be looking for something balanced. So maybe the yogurt with berries and chia seeds is a good option. Let me confirm the calories again. Greek yogurt (100), berries (50), chia seeds (30). Total 180. That's under 200. If they add a small piece of fruit or a little nut butter, maybe a teaspoon, that's another 50, but need to stay under. Alternatively, maybe a slice of whole-grain toast with a tiny bit of peanut butter. But that would add more calories.\n",
      "\n",
      "Another idea: avocado toast. Half an avocado is about 120 calories, a slice of whole-grain toast is 80. That's 200. But maybe the user wants something with more protein. So maybe add a poached egg on top. That's 70 calories. Total 270. Too high. So maybe skip the egg. Just avocado toast with a sprinkle of salt and pepper. 200 calories exactly. That's a good option.\n",
      "\n",
      "Wait, but the user might prefer variety. So offering a couple of options would be better. Let me outline a couple of options with their calorie counts. Make sure each is under 200. Also, mention the benefits like fiber, protein, etc.\n",
      "\n",
      "I should also consider dietary restrictions, but the user didn't mention any. So assuming no restrictions. Also, maybe mention that they can adjust portions if needed. For example, using a smaller amount of something if they want to stay under 200.\n",
      "\n",
      "Let me structure the answer with a couple of options, each with ingredients and calorie breakdown. Also, maybe a note about adjusting as needed. Avoid using any high-calorie ingredients like cheese, butter, or full-fat dairy unless in small amounts. Focus on whole foods, lean proteins, and low-calorie veggies and fruits.\n",
      "\n",
      "Double-check the calorie counts. For example, a large egg is about 70 calories, small is 60. Whole-grain toast is around 70-80 per slice. Almond butter is about 100 calories per tablespoon, which is too much. So maybe a teaspoon is 30 calories. Chia seeds: a tablespoon is about 120, so a teaspoon is 40. Wait, maybe I miscalculated earlier. Let me verify:\n",
      "\n",
      "- Large egg: 70 calories\n",
      "- Whole-grain toast (1 slice): ~70\n",
      "- Almond butter (1 tsp): ~30\n",
      "- Greek yogurt (1 cup, low-fat): ~100\n",
      "- Blueberries (1/2 cup): ~40\n",
      "- Chia seeds (1 tbsp): ~120, so 1 tsp is ~40\n",
      "- Oats (1/2 cup dry): ~150\n",
      "- Banana (1/2 medium): ~50\n",
      "- Avocado (1/2 medium): ~120\n",
      "- Whole-grain toast (1 slice): ~70\n",
      "\n",
      "So for the avocado toast: 120 + 70 = 190. Under 200. Perfect. Maybe add a poached egg for extra protein but that would push it over. So maybe just the avocado toast.\n",
      "\n",
      "Another option: 1 egg + 1 slice toast + veggies. 70 + 70 = 140. Add veggies which are low calorie. That's under 200.\n",
      "\n",
      "Or the Greek yogurt with berries and chia seeds: 100 + 40 + 40 = 180. That's good.\n",
      "\n",
      "So the answer should present a couple of these options with clear instructions and calorie counts. Also, mention that they can adjust portions or ingredients as needed. Make sure to keep it simple and easy to follow.\n",
      "<|end|><|start|>assistant<|channel|>final<|message|>Here are two healthy, low-calorie breakfast options (under 200 calories) that are balanced and nutrient-dense:\n",
      "\n",
      "---\n",
      "\n",
      "### **Option 1: Greek Yogurt Parfait (≈180 calories)**  \n",
      "**Ingredients:**  \n",
      "- 1 cup low-fat Greek yogurt (≈100 calories)  \n",
      "- 1/2 cup fresh berries (blueberries, strawberries, or raspberries) (≈40 calories)  \n",
      "- 1 tsp chia seeds (≈40 calories)  \n",
      "- 1 tbsp granola (optional, for crunch; ≈50 calories if included)  \n",
      "\n",
      "**Instructions:**  \n",
      "1. Layer yogurt, berries, and chia seeds in a bowl or glass.  \n",
      "2. Top with a small amount of granola (skip if keeping it strictly under 200).  \n",
      "\n",
      "**Why it works:**  \n",
      "- **Protein-rich** (from Greek yogurt and chia seeds) for satiety.  \n",
      "- **Fiber** from berries and chia seeds to support digestion.  \n",
      "- **Low in sugar** if using plain yogurt and fresh fruit.  \n",
      "\n",
      "---\n",
      "\n",
      "### **Option 2: Avocado Toast with Egg (≈190 calories)**  \n",
      "**Ingredients:**  \n",
      "- 1 slice whole-grain bread (≈70 calories)  \n",
      "- 1/2 medium avocado, mashed (≈120 calories)  \n",
      "- 1 poached egg (optional, ≈70 calories; skip to stay under 200)  \n",
      "\n",
      "**Instructions:**  \n",
      "1. Toast the bread.  \n",
      "2. Spread mashed avocado on top.  \n",
      "3. Add a poached or fried egg for extra protein (optional).  \n",
      "\n",
      "**Why it works:**  \n",
      "- **Healthy fats** from avocado for heart health.  \n",
      "- **Complex carbs** from whole-grain bread for sustained energy.  \n",
      "- **Lean protein** from the egg (if included).  \n",
      "\n",
      "---\n",
      "\n",
      "### **Tips to Stay Under 200 Calories:**  \n",
      "- Use **non-fat milk** instead of creamer in smoothies or oatmeal.  \n",
      "- Opt for **fresh fruit** over dried (which is calorie-dense).  \n",
      "- Use **spices** (e.g., cinnamon, pepper) instead of added sugars or oils.  \n",
      "\n",
      "Let me know if you'd like more options!<|return|>\n"
     ]
    }
   ],
   "source": [
    "messages = dataset[0][\"messages\"]\n",
    "conversation = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "print(conversation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d237f24",
   "metadata": {
    "id": "9d237f24"
   },
   "source": [
    "## 3&nbsp;&nbsp;Prepare the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b6cc8cf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411,
     "referenced_widgets": [
      "bec979622f35461b88b6218ecff08d1d",
      "50a368dd3f8c48c889d6aa271a4560b4",
      "48bdc6ee888f45e699c7ae22c6c7973e",
      "18cdf2f4cda14beab466e66d9b5637c0",
      "8444c8b2148b4348b483db0d9a6833ce",
      "3e132807d1fe49be93ededc1c38504a6",
      "eebb74faae0f46d1b78c761a02b0b484",
      "6d1b5b8629fb4ea98e2ad72387fb76b7",
      "ccae9bc300694095ab5eacf559021269",
      "fa326aee2e3748a583e951cc97ed14f3",
      "32ac4c0474b641edae3e0be80dcdf795"
     ]
    },
    "id": "4b6cc8cf",
    "outputId": "ac06e33d-aab2-4af0-d4d7-acf39952936b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf0fc503f1f14790b3257230a02633cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, Mxfp4Config\n",
    "\n",
    "quantization_config = Mxfp4Config(dequantize=True)\n",
    "model_kwargs = dict(\n",
    "    attn_implementation=\"eager\",\n",
    "    torch_dtype=torch.bfloat16, # float16 for colab [although will OOM on T4], bfloat16 for ampere, hopper or later\n",
    "    quantization_config=quantization_config, # comment out for full fine-tuning\n",
    "    use_cache=False,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"openai/gpt-oss-20b\", **model_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8a840a7-2b67-4ed1-a36c-084a8af29039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Aug  6 11:26:43 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA H100 80GB HBM3          On  |   00000000:AB:00.0 Off |                    0 |\n",
      "| N/A   33C    P0            146W /  700W |   44331MiB /  81559MiB |      5%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ac4356d",
   "metadata": {
    "id": "2ac4356d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "systemYou are ChatGPT, a large language model trained by OpenAI.\n",
      "Knowledge cutoff: 2024-06\n",
      "Current date: 2025-08-06\n",
      "\n",
      "Reasoning: medium\n",
      "\n",
      "# Valid channels: analysis, commentary, final. Channel must be included for every message.user¿Cuál es el capital de Australia?assistantanalysisThe user asks in Spanish: \"¿Cuál es el capital de Australia?\" That's the capital of Australia: Canberra. They want the capital. We'll answer: Canberra.\n",
      "\n",
      "Should also confirm: The capital of Australia is Canberra. We'll respond in Spanish. The conversation is short, simple. We'll provide the answer.assistantfinalLa capital de Australia es **Canberra**.\n"
     ]
    }
   ],
   "source": [
    "messages = [{\"role\": \"user\", \"content\": \"¿Cuál es el capital de Australia?\"}]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "\n",
    "output_ids = model.generate(input_ids, max_new_tokens=512)\n",
    "print(tokenizer.decode(output_ids[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d5a7ae",
   "metadata": {
    "id": "f4d5a7ae"
   },
   "source": [
    "### LoRA configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64f48c94",
   "metadata": {
    "id": "64f48c94"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/layer.py:159: UserWarning: Unsupported layer type '<class 'transformers.models.gpt_oss.modeling_gpt_oss.GptOssExperts'>' encountered, proceed at your own risk.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 15,040,512 || all params: 20,929,797,696 || trainable%: 0.0719\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# peft_config = LoraConfig(\n",
    "#     r=8,\n",
    "#     lora_alpha=16,\n",
    "#     target_modules=[\n",
    "#         \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "#         # \"mlp.experts.gate_up_proj\", \"mlp.experts.gate_down_proj\" # doesn't work\n",
    "#     ]\n",
    "# )\n",
    "# peft_model = get_peft_model(model, peft_config)\n",
    "# peft_model.print_trainable_parameters()\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=\"all-linear\",\n",
    "    # target_modules=[\"o_proj\",\"v_proj\",\"q_proj\",\"k_proj\"]\n",
    "    target_parameters=[\n",
    "        \"7.mlp.experts.gate_up_proj\",\n",
    "        \"7.mlp.experts.down_proj\",\n",
    "        \"15.mlp.experts.gate_up_proj\",\n",
    "        \"15.mlp.experts.down_proj\",\n",
    "        \"23.mlp.experts.gate_up_proj\",\n",
    "        \"23.mlp.experts.down_proj\",\n",
    "    ],\n",
    ")\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1667e28-2250-49e3-baa2-622660b48c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"\\n--- Matched Modules for LoRA ---\")\n",
    "# for name, module in model.named_modules():\n",
    "#     for target in peft_config.target_modules:\n",
    "#         if target in name:\n",
    "#             print(name)\n",
    "\n",
    "# print(\"\\n--- Trainable Parameters (LoRA-Injected) ---\")\n",
    "# for name, param in peft_model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffae19dc",
   "metadata": {
    "id": "ffae19dc"
   },
   "source": [
    "## 4&nbsp;&nbsp;Fine‑tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79211018-8ece-402b-b26e-9cc2252b2a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = \"oss-multi-lingual\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d9d12fc-838c-45fd-b0ee-7f69299b3cbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-b88d785787698772\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-b88d785787698772\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./logs --port 6006 --bind_all --reload_interval 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb40ac29",
   "metadata": {
    "id": "bb40ac29"
   },
   "outputs": [],
   "source": [
    "from trl import SFTConfig\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    hub_model_id=f\"Trelis/{run_name}\",  # <--- controls where the model is pushed\n",
    "    learning_rate=2e-4,\n",
    "    do_eval=True,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=0.2,\n",
    "    logging_dir=f\"logs/{run_name}\",\n",
    "    gradient_checkpointing=True,\n",
    "    # num_train_epochs=1,\n",
    "    max_steps=4,\n",
    "    logging_steps=0.05,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=int(batch_size / 4),\n",
    "    gradient_accumulation_steps=int(32/batch_size),\n",
    "    max_length=2048,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine_with_min_lr\",\n",
    "    lr_scheduler_kwargs={\"min_lr_rate\": 0.1},\n",
    "    output_dir=\"outputs/gpt-oss-20b-multilingual-reasoner\",\n",
    "    # report_to=\"trackio\", # and use 'uv pip install trackio -qU', followed by 'trackio show'\n",
    "    report_to=\"tensorboard\",\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07b6f961",
   "metadata": {
    "id": "07b6f961"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 02:13, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.876900</td>\n",
       "      <td>2.188597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.981900</td>\n",
       "      <td>2.021582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.838500</td>\n",
       "      <td>1.930961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.656100</td>\n",
       "      <td>1.893667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4, training_loss=1.8383430540561676, metrics={'train_runtime': 167.0626, 'train_samples_per_second': 0.766, 'train_steps_per_second': 0.024, 'total_flos': 2.3817115002710016e+16, 'train_loss': 1.8383430540561676})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "068a8685",
   "metadata": {
    "id": "068a8685"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Trelis/oss-multi-lingual/commit/f3b46c91d46310536fecf957bd23d34d9f4e725b', commit_message='End of training', commit_description='', oid='f3b46c91d46310536fecf957bd23d34d9f4e725b', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Trelis/oss-multi-lingual', endpoint='https://huggingface.co', repo_type='model', repo_id='Trelis/oss-multi-lingual'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(training_args.output_dir)\n",
    "trainer.push_to_hub(dataset_name=\"HuggingFaceH4/Multilingual-Thinking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7672c58b",
   "metadata": {
    "id": "7672c58b"
   },
   "source": [
    "## 5&nbsp;&nbsp;Inference\n",
    "**YOU MAY NEED TO RESTART THE KERNEL HERE TO CLEAR THE GPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00aa0a20",
   "metadata": {
    "id": "00aa0a20"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MXFP4 quantization requires triton >= 3.4.0 and triton_kernels installed, we will default to dequantizing the model to bf16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05c51a516e03491d89a977d918325b02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/layer.py:159: UserWarning: Unsupported layer type '<class 'transformers.models.gpt_oss.modeling_gpt_oss.GptOssExperts'>' encountered, proceed at your own risk.\n",
      "  warnings.warn(\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "systemYou are ChatGPT, a large language model trained by OpenAI.\n",
      "Knowledge cutoff: 2024-06\n",
      "Current date: 2025-08-06\n",
      "\n",
      "Reasoning: medium\n",
      "\n",
      "# Valid channels: analysis, commentary, final. Channel must be included for every message.developer# Instructions\n",
      "\n",
      "reasoning language: Germanuser¿Cuál es el capital de Australia?assistantanalysisThe user asks: \"¿Cuál es el capital de Australia?\" Spanish: \"What is the capital of Australia?\" The answer: Canberra. The user didn't specify language. The conversation: user in Spanish. The assistant should respond in Spanish. The user didn't ask for code or anything else. So respond: \"La capital de Australia es Canberra.\" The user is Spanish, so answer in Spanish. Also maybe mention that Canberra is the capital. So final.assistantfinalLa capital de Australia es Canberra.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai/gpt-oss-20b\")\n",
    "\n",
    "model_kwargs = dict(attn_implementation=\"eager\", torch_dtype=\"auto\", use_cache=True, device_map=\"auto\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"openai/gpt-oss-20b\", **model_kwargs).cuda()\n",
    "\n",
    "peft_model_id = \"outputs/gpt-oss-20b-multilingual-reasoner\"\n",
    "model = PeftModel.from_pretrained(base_model, peft_model_id)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Push to merged.\n",
    "# model.push_to_hub(f\"Trelis/{run_name}-merged)\n",
    "\n",
    "REASONING_LANGUAGE = \"German\"\n",
    "SYSTEM_PROMPT = f\"reasoning language: {REASONING_LANGUAGE}\"\n",
    "USER_PROMPT = \"¿Cuál es el capital de Australia?\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": USER_PROMPT},\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "\n",
    "gen_kwargs = {\"max_new_tokens\": 512, \"do_sample\": True, \"temperature\": 0.6}\n",
    "output_ids = model.generate(input_ids, **gen_kwargs)\n",
    "print(tokenizer.decode(output_ids[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef7be0db",
   "metadata": {
    "id": "ef7be0db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "systemYou are ChatGPT, a large language model trained by OpenAI.\n",
      "Knowledge cutoff: 2024-06\n",
      "Current date: 2025-08-06\n",
      "\n",
      "Reasoning: medium\n",
      "\n",
      "# Valid channels: analysis, commentary, final. Channel must be included for every message.developer# Instructions\n",
      "\n",
      "reasoning language: FrenchuserC'est quoi la langue nationale du Canada?assistantanalysisThe user asks in French: \"C'est quoi la langue nationale du Canada?\" They likely want the answer: Canada has two official languages: English and French. The question: \"C'est quoi la langue nationale du Canada?\" The correct answer: Canada has two official languages: English and French. There's no single national language; both are official. Provide explanation.assistantfinalLe Canada n’a pas de « langue nationale » unique.  \n",
      "Le pays est officiellement bilingue : **l’anglais et le français** sont les deux langues officielles au niveau fédéral.  \n",
      "\n",
      "- **Anglais** : langue majoritaire dans la majorité des provinces (sauf Québec, New Brunswick, etc.).  \n",
      "- **Français** : langue officielle et majoritaire au Québec, et également officielle dans le Nouveau-Brunswick, la région de l’Île-du-Prince-Édouard et certains territoires (Yukon, Territoires du Nord-Ouest, Nunavut).\n",
      "\n",
      "Cette double officialité est inscrite dans la Constitution canadienne (la Loi sur l’anglais et le français, 1985) et elle guide la politique gouvernementale, l’éducation, les services publics, etc.  \n",
      "Ainsi, si vous cherchez une langue nationale, il faut dire que le Canada est officiellement bilingue : anglais et français.\n"
     ]
    }
   ],
   "source": [
    "# You need to train more for this to work in chinese.\n",
    "\n",
    "REASONING_LANGUAGE = \"French\"\n",
    "SYSTEM_PROMPT = f\"reasoning language: {REASONING_LANGUAGE}\"\n",
    "USER_PROMPT = \"C'est quoi la langue nationale du Canada?\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": USER_PROMPT},\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "\n",
    "output_ids = model.generate(input_ids, **gen_kwargs)\n",
    "print(tokenizer.decode(output_ids[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf12dcfc",
   "metadata": {
    "id": "bf12dcfc"
   },
   "source": [
    "## 6&nbsp;&nbsp;Conclusion\n",
    "\n",
    "You fine‑tuned **`openai/gpt-oss-20b`** to reason in multiple languages using **TRL** + **LoRA** and the *Multilingual‑Thinking* dataset.  \n",
    "Adapt these steps to your own data and build models that think in any language you need!\n",
    "\n",
    "For more advanced scripts, check out [Youtube.com/@TrelisResearch].\n",
    "\n",
    "---\n",
    "\n",
    "*Notebook generated by Trelis.*"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "18cdf2f4cda14beab466e66d9b5637c0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fa326aee2e3748a583e951cc97ed14f3",
      "placeholder": "​",
      "style": "IPY_MODEL_32ac4c0474b641edae3e0be80dcdf795",
      "value": " 1/3 [00:17&lt;00:21, 10.92s/it]"
     }
    },
    "32ac4c0474b641edae3e0be80dcdf795": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3e132807d1fe49be93ededc1c38504a6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "48bdc6ee888f45e699c7ae22c6c7973e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6d1b5b8629fb4ea98e2ad72387fb76b7",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ccae9bc300694095ab5eacf559021269",
      "value": 1
     }
    },
    "50a368dd3f8c48c889d6aa271a4560b4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3e132807d1fe49be93ededc1c38504a6",
      "placeholder": "​",
      "style": "IPY_MODEL_eebb74faae0f46d1b78c761a02b0b484",
      "value": "Loading checkpoint shards:  33%"
     }
    },
    "6d1b5b8629fb4ea98e2ad72387fb76b7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8444c8b2148b4348b483db0d9a6833ce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bec979622f35461b88b6218ecff08d1d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_50a368dd3f8c48c889d6aa271a4560b4",
       "IPY_MODEL_48bdc6ee888f45e699c7ae22c6c7973e",
       "IPY_MODEL_18cdf2f4cda14beab466e66d9b5637c0"
      ],
      "layout": "IPY_MODEL_8444c8b2148b4348b483db0d9a6833ce"
     }
    },
    "ccae9bc300694095ab5eacf559021269": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "eebb74faae0f46d1b78c761a02b0b484": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fa326aee2e3748a583e951cc97ed14f3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
